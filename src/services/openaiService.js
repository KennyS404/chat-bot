import OpenAI from 'openai';
import { config } from '../config.js';
import { logger } from '../logger.js';

export class OpenAIService {
  constructor() {
    // Check if OpenAI API key is configured (required at least for fallback)
    if (!config.openai.apiKey && !config.deepseek.apiKey) {
      logger.error('‚ùå Nenhuma chave de API configurada!');
      logger.error('   Configure OPENAI_API_KEY ou DEEPSEEK_API_KEY no arquivo .env');
      throw new Error('API key missing');
    }

    // Selecionar provedor conforme o modo definido nas configura√ß√µes
    let clientOptions = {};
    switch (config.ai.mode) {
      case 'deepseek': {
        clientOptions = {
          baseURL: config.deepseek.baseURL,
          apiKey: config.deepseek.apiKey,
        };
        break;
      }
      case 'hybrid':
      case 'openai':
      default: {
        clientOptions = {
          apiKey: config.openai.apiKey,
        };
        break;
      }
    }

    // Criar cliente
    this.openai = new OpenAI(clientOptions);

    // Testar a chave da API ao inicializar
    this.testAPIKey();
  }

  async testAPIKey() {
    try {
      logger.info('üîë Testando chave da API OpenAI...');
      
      // Testar acesso √† API de modelos
      const models = await this.openai.models.list();
      logger.info('‚úÖ Acesso √† API de modelos OK');
      logger.info(`üìã Modelos dispon√≠veis: ${models.data.map(m => m.id).join(', ')}`);

      // Testar acesso √† API de √°udio
      const testText = 'Teste de permiss√µes da API.';
      const testAudio = await this.openai.audio.speech.create({
        model: config.openai.ttsModel,
        voice: config.openai.ttsVoice,
        input: testText,
      });
      logger.info('‚úÖ Acesso √† API de √°udio OK');

      // Verificar modelo Whisper
      if (models.data.some(m => m.id === config.openai.whisperModel)) {
        logger.info(`‚úÖ Modelo ${config.openai.whisperModel} dispon√≠vel`);
      } else {
        logger.warn(`‚ö†Ô∏è Modelo ${config.openai.whisperModel} n√£o encontrado na lista de modelos dispon√≠veis`);
      }

      logger.info('‚úÖ Teste de API conclu√≠do com sucesso!');
    } catch (error) {
      logger.error('‚ùå Erro ao testar API:', error);
      logger.error('Detalhes do erro:', {
        message: error.message,
        status: error.status,
        code: error.code,
        type: error.type,
        response: error.response?.data
      });

      if (error.status === 401) {
        logger.error('‚ùå Chave da API inv√°lida ou expirada');
        logger.error('   Verifique se a chave est√° correta e tem as permiss√µes necess√°rias');
        logger.error('   Obtenha uma nova chave em: https://platform.openai.com/api-keys');
      } else if (error.status === 429) {
        logger.error('‚ùå Limite de requisi√ß√µes excedido');
        logger.error('   Aguarde alguns minutos e tente novamente');
      } else if (error.status === 403) {
        logger.error('‚ùå Permiss√µes insuficientes');
        logger.error('   Verifique se sua conta tem acesso aos modelos necess√°rios');
        logger.error('   Modelos necess√°rios: whisper-1, tts-1');
      }

      throw error;
    }
  }

  async transcribeAudio(audioBuffer) {
    try {
      logger.info('Preparando √°udio para transcri√ß√£o...');
      const file = new File([audioBuffer], 'audio.mp3', { type: 'audio/mp3' });
      logger.info(`Arquivo preparado - Tamanho: ${file.size} bytes`);
      
      logger.info(`Enviando para Whisper (modelo: ${config.openai.whisperModel})...`);
      const transcription = await this.openai.audio.transcriptions.create({
        file: file,
        model: config.openai.whisperModel,
        language: 'pt',
        response_format: 'text',
      });

      logger.info('Transcri√ß√£o recebida:', transcription);
      return transcription;
    } catch (error) {
      logger.error('Erro ao transcrever √°udio:', error);
      logger.error('Detalhes do erro:', {
        message: error.message,
        status: error.status,
        code: error.code,
        type: error.type,
        response: error.response?.data,
        stack: error.stack,
        cause: error.cause,
        audioSize: audioBuffer.length,
        audioType: typeof audioBuffer,
        isBuffer: Buffer.isBuffer(audioBuffer)
      });
      
      // Verificar se √© um erro de API
      if (error.response?.data) {
        logger.error('Resposta da API OpenAI:', error.response.data);
      }
      
      // Verificar se √© um erro de rede
      if (error.code === 'ECONNREFUSED' || error.code === 'ETIMEDOUT') {
        logger.error('Erro de conex√£o com a API OpenAI');
      }
      
      throw error;
    }
  }

  async correctGrammar(text) {
    try {
      logger.info(`Analisando gram√°tica do texto: "${text}"`);
      logger.info(`Usando modelo: ${config.openai.gptModel}`);
      
      const response = await this.openai.chat.completions.create({
        model: config.openai.gptModel,
        messages: [
          {
            role: 'system',
            content: `Voc√™ √© um corretor gramatical especializado em portugu√™s brasileiro. 
            Sua tarefa √© analisar o texto transcrito de um √°udio e:
            1. Identificar erros gramaticais, ortogr√°ficos e de concord√¢ncia
            2. Sugerir uma vers√£o corrigida do texto
            3. Manter o sentido original da mensagem
            4. Ser conciso e direto na resposta
            
            Formato da resposta:
            - Se houver corre√ß√µes: "Voc√™ quis dizer: [texto corrigido]"
            - Se n√£o houver corre√ß√µes: "‚úÖ Seu √°udio est√° correto!"
            
            Importante: Mantenha a naturalidade da fala, apenas corrija erros evidentes.`
          },
          {
            role: 'user',
            content: text
          }
        ],
        temperature: 0.3,
        max_tokens: 500,
      });

      const correction = response.choices[0].message.content;
      logger.info('Corre√ß√£o recebida:', correction);
      return correction;
    } catch (error) {
      logger.error('Erro ao corrigir gram√°tica:', error);
      logger.error('Detalhes do erro:', {
        message: error.message,
        status: error.status,
        code: error.code,
        type: error.type
      });
      throw error;
    }
  }

  async textToSpeech(text) {
    try {
      logger.info('Gerando √°udio da resposta...');
      logger.info(`Texto para TTS: "${text}"`);
      const startTime = Date.now();
      
      // Usar a voz mais natural dispon√≠vel (nova voz da OpenAI)
      const response = await this.openai.audio.speech.create({
        model: config.openai.ttsModel, // tts-1 √© mais r√°pido que tts-1-hd
        voice: config.openai.ttsVoice, // nova √© a voz mais natural em portugu√™s
        input: text,
        speed: 1.0, // Velocidade normal
        response_format: 'mp3' // Especificar formato MP3 explicitamente
      });

      logger.info('Resposta TTS recebida, convertendo para buffer...');
      
      // Converter a resposta em buffer
      const arrayBuffer = await response.arrayBuffer();
      const audioBuffer = Buffer.from(arrayBuffer);
      
      const duration = Date.now() - startTime;
      logger.info(`√Åudio gerado em ${duration}ms - Tamanho: ${audioBuffer.length} bytes`);
      logger.info(`Primeiros bytes do √°udio: ${audioBuffer.slice(0, 10).toString('hex')}`);
      
      // Verificar se √© um MP3 v√°lido (deve come√ßar com ID3 ou FF FB)
      const header = audioBuffer.slice(0, 3).toString();
      const isValidMP3 = header === 'ID3' || audioBuffer[0] === 0xFF;
      logger.info(`Valida√ß√£o MP3 - Header: ${header}, √â v√°lido: ${isValidMP3}`);
      
      return audioBuffer;
    } catch (error) {
      logger.error('Erro ao gerar √°udio:', error);
      logger.error('Detalhes do erro:', {
        message: error.message,
        status: error.status,
        code: error.code,
        type: error.type,
        response: error.response?.data
      });
      throw error;
    }
  }

  async processAudio(audioBuffer) {
    try {
      // Transcrever o √°udio
      const transcription = await this.transcribeAudio(audioBuffer);
      logger.info('Transcri√ß√£o:', transcription);

      // Corrigir gram√°tica e gerar √°udio em paralelo para m√°xima velocidade
      const correctionPromise = this.correctGrammar(transcription);
      
      // Aguardar a corre√ß√£o primeiro
      const correction = await correctionPromise;
      const hasCorrections = !correction.includes('‚úÖ');
      
      // Gerar √°udio apenas se houver corre√ß√µes e estiver habilitado
      let audioResponse = null;
      if (hasCorrections && config.app.enableAudioResponse) {
        // Extrair apenas o texto corrigido para o √°udio
        const correctedText = correction.replace('Voc√™ quis dizer: ', '');
        
        // Gerar √°udio em paralelo com o envio da mensagem de texto
        audioResponse = await this.textToSpeech(correctedText);
      }
      
      return {
        transcription,
        correction,
        hasCorrections,
        audioResponse
      };
    } catch (error) {
      logger.error('Erro ao processar √°udio:', error);
      throw error;
    }
  }
} 